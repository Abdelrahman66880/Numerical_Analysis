{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "split-mercy",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "a34b810972fc06c80ac9d379a9f33ca4",
          "grade": false,
          "grade_id": "cell-1fc84fa265289359",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "split-mercy"
      },
      "source": [
        "# CSE213 - Numerical Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 5 - Numerical Differentation"
      ],
      "metadata": {
        "id": "3eL2ZgKdAe_u"
      },
      "id": "3eL2ZgKdAe_u"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Numerical Differentation Approximation Formulas"
      ],
      "metadata": {
        "id": "qNzURLvptv8l"
      },
      "id": "qNzURLvptv8l"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### First Derivative Forward Pass Difference Formula\n",
        "\n",
        "Given a function $f(x)$, we want to calculate its derivative $f'(x_0)$ at point $x_0$. For a small step $h$, the derivative can be approximated using the following forward difference formula:\n",
        "\n",
        "$$f'(x) ≈ \\frac{f(x + h) - f(x)}{h}$$\n",
        "\n",
        "### First Derivative Centred Divided-Difference Formula\n",
        "\n",
        "Given the same conditions as the forward pass difference formula, the centred divided-difference formula is as follows:\n",
        "\n",
        "$$f'(x) \\approx \\frac{f(x + h) - f(x - h)}{2h}$$\n",
        "\n",
        "### Second Derivative Centred Divided-Difference Formula\n",
        "\n",
        "Applying same conditions in the above two formulas, the centred divided-difference formula for second derivative is as follows:\n",
        "\n",
        "$$\n",
        "f''(x) \\approx \\frac{f(x+h)-2 f(x)+f(x-h)}{h^2}\n",
        "$$"
      ],
      "metadata": {
        "id": "N8RS06Eit3Y3"
      },
      "id": "N8RS06Eit3Y3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 1\n",
        "Apply the approximation formulas of both the derivative and 2nd-derivative of $f(x) = x^{3} e^{–0.5x}$ at $x = 0.8$. **Explain why the why the approximation breaks down for very small step $h$**?"
      ],
      "metadata": {
        "id": "YvcHcsts5wyu"
      },
      "id": "YvcHcsts5wyu"
    },
    {
      "cell_type": "code",
      "source": [
        "from prettytable import PrettyTable as pt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "nbKC1XD6e_lv"
      },
      "id": "nbKC1XD6e_lv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_forw = lambda f, x, h: (f(x + h) - f(x)) / h\n",
        "df_cdiv = lambda f, x, h: (f(x + h) - f(x - h)) / (2 * h)\n",
        "df2_cdiv = lambda f, x, h: (f(x + h) - 2*f(x) + f(x - h)) / (h ** 2)\n",
        "f1 = lambda x: x ** 3 * np.exp(-0.5* x)\n",
        "f1_df = lambda x: 3 * x ** 2 * np.exp(-x / 2) - ( x ** 3 * np.exp(-x / 2)) / 2\n",
        "f1_df2 = lambda x: (x * (x ** 2 -12 * x + 24) * np.exp(-x / 2)) / 4"
      ],
      "metadata": {
        "id": "PPukBqnUe-Xs"
      },
      "id": "PPukBqnUe-Xs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = pt([\"h\", \"df_f\", \"Error\", \"df_c\", \"Error_\", \"df2_c\", \"Error__\"])\n",
        "x0 = 0.8\n",
        "val, val_df, val_df2 = f1(x0), f1_df(x0), f1_df2(x0)\n",
        "\n",
        "for i in range(1, 10):\n",
        "    h = 10 ** (-i)\n",
        "    df_f, df_c, df2_c = df_forw(f1, x0, h), df_cdiv(f1, x0, h), df2_cdiv(f1, x0, h)\n",
        "    results.add_row([h, df_f, abs(val_df - df_f), df_c, abs(val_df - df_c), df2_c, abs(val_df2 - df2_c)])\n",
        "\n",
        "print(f\"True Derivative: {val_df}\\nTrue Second Derivative: {val_df2}\")\n",
        "print(results)"
      ],
      "metadata": {
        "id": "P4M8qW8i5xrh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e263c9bb-a051-4287-b774-54ce9cc6d183"
      },
      "id": "P4M8qW8i5xrh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True Derivative: 1.1154125566033042\n",
            "True Second Derivative: 2.016322698475203\n",
            "+--------+--------------------+------------------------+--------------------+------------------------+--------------------+------------------------+\n",
            "|   h    |        df_f        |         Error          |        df_c        |         Error_         |       df2_c        |        Error__         |\n",
            "+--------+--------------------+------------------------+--------------------+------------------------+--------------------+------------------------+\n",
            "|  0.1   | 1.2162705896202541 |  0.10085803301694996   | 1.1156145387937701 | 0.00020198219046596577 | 2.0131210165296785 |  0.003201681945524726  |\n",
            "|  0.01  | 1.1254959769191109 |  0.010083420315806668  | 1.1154145234108015 | 1.9668074973111516e-06 | 2.0162907016618714 |  3.19968133317694e-05  |\n",
            "| 0.001  | 1.1164207374552704 | 0.0010081808519661895  | 1.1154125762660727 | 1.9662768524497665e-08 |  2.01632237839533  | 3.2007987327276055e-07 |\n",
            "| 0.0001 | 1.1155133729340294 | 0.00010081633072522855 | 1.1154125567994222 | 1.9611801072016988e-10 | 2.0163226921443567 | 6.330846513691313e-09  |\n",
            "| 1e-05  | 1.1154226382148469 | 1.0081611542700486e-05 | 1.1154125566043005 | 9.963141422986155e-13  | 2.0163221092772683 | 5.891979348859877e-07  |\n",
            "| 1e-06  | 1.1154135647895025 | 1.0081861983035623e-06 | 1.115412556651485  | 4.818079268886777e-11  | 2.016276035021747  | 4.6663453456385895e-05 |\n",
            "| 1e-07  | 1.1154126566825795 | 1.0007927531141547e-07 |  1.11541255592984  |  6.73464173317484e-10  | 2.0150547896946596 |  0.001267908780543614  |\n",
            "| 1e-08  | 1.1154125623136224 | 5.710318218277166e-09  | 1.1154125595380648 | 2.9347606567142748e-09 | 0.5551115123125782 |   1.461211186162625    |\n",
            "| 1e-09  | 1.115412540109162  | 1.6494142274225965e-08 | 1.1154125123535863 | 4.424971788985488e-08  | 55.51115123125782  |   53.49482853278262    |\n",
            "+--------+--------------------+------------------------+--------------------+------------------------+--------------------+------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sympy as sp\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "# Define the symbol and the function\n",
        "x = sp.Symbol('x')\n",
        "f = x**3 * sp.exp(-0.5*x)\n",
        "\n",
        "# Compute the first and second derivatives symbolically\n",
        "f_prime = sp.diff(f, x)\n",
        "f_double_prime = sp.diff(f_prime, x)\n",
        "\n",
        "# Define the lambdified versions of the symbolic functions for numerical evaluation\n",
        "f1 = sp.lambdify(x, f)\n",
        "f1_df = sp.lambdify(x, f_prime)\n",
        "f1_df2 = sp.lambdify(x, f_double_prime)\n",
        "\n",
        "# Define the forward and central difference methods\n",
        "df_forw = lambda f, x, h: (f(x + h) - f(x)) / h\n",
        "df_cdiv = lambda f, x, h: (f(x + h) - f(x - h)) / (2 * h)\n",
        "df2_cdiv = lambda f, x, h: (f(x + h) - 2*f(x) + f(x - h)) / (h ** 2)"
      ],
      "metadata": {
        "id": "U5p4X7GmRxWI"
      },
      "id": "U5p4X7GmRxWI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the table for results\n",
        "results = PrettyTable([\"h\", \"df_f\", \"Error\", \"df_c\", \"Error_\", \"df2_c\", \"Error__\"])\n",
        "\n",
        "# Evaluate the true values of the derivatives at x = 0.8\n",
        "x0 = 0.8\n",
        "val, val_df, val_df2 = f1(x0), f1_df(x0), f1_df2(x0)\n",
        "# Loop over different step sizes\n",
        "for i in range(1, 10):\n",
        "    h = 10 ** (-i)\n",
        "    df_f, df_c, df2_c = df_forw(f1, x0, h), df_cdiv(f1, x0, h), df2_cdiv(f1, x0, h)\n",
        "    # Compute the errors\n",
        "    error_df = abs(val_df - df_f)\n",
        "    error_df_c = abs(val_df - df_c)\n",
        "    error_df2_c = abs(val_df2 - df2_c)\n",
        "    # Add the results to the table\n",
        "    results.add_row([h, df_f, error_df, df_c, error_df_c, df2_c, error_df2_c])\n",
        "\n",
        "# Print the true values of the derivatives\n",
        "print(f\"True Derivative: {val_df}\\nTrue Second Derivative: {val_df2}\")\n",
        "\n",
        "# Print the table of results\n",
        "print(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-loQnFBRxbd",
        "outputId": "e40acb88-77de-4a95-95ca-b99b7fa27441"
      },
      "id": "P-loQnFBRxbd",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True Derivative: 1.1154125566033042\n",
            "True Second Derivative: 2.0163226984752036\n",
            "+--------+--------------------+------------------------+--------------------+------------------------+--------------------+------------------------+\n",
            "|   h    |        df_f        |         Error          |        df_c        |         Error_         |       df2_c        |        Error__         |\n",
            "+--------+--------------------+------------------------+--------------------+------------------------+--------------------+------------------------+\n",
            "|  0.1   | 1.2162705896202541 |  0.10085803301694996   | 1.1152212816586167 | 0.00019127494468751394 | 2.0131210165296785 |  0.00320168194552517   |\n",
            "|  0.01  | 1.1254959769191109 |  0.010083420315806668  | 1.1154105908665297 | 1.9657367744674303e-06 | 2.0162907016618714 | 3.199681333221349e-05  |\n",
            "| 0.001  | 1.1164207374552704 | 0.0010081808519661895  | 1.1154125369406362 | 1.9662667938291634e-08 |  2.01632237839533  | 3.2007987371684976e-07 |\n",
            "| 0.0001 | 1.1155133729340294 | 0.00010081633072522855 | 1.1154125564066766 | 1.9662760308847282e-10 | 2.0163226921443567 | 6.330846957780523e-09  |\n",
            "| 1e-05  | 1.1154226382148469 | 1.0081611542700486e-05 | 1.1154125566013378 | 1.9664270212160773e-12 | 2.0163221092772683 | 5.891979353300769e-07  |\n",
            "| 1e-06  | 1.1154135647895025 | 1.0081861983035623e-06 | 1.115412556603284  | 2.020605904817785e-14  | 2.016276035021747  | 4.6663453456829984e-05 |\n",
            "| 1e-07  | 1.1154126566825795 | 1.0007927531141547e-07 | 1.115412556603304  | 2.220446049250313e-16  | 2.0150547896946596 |  0.001267908780544058  |\n",
            "| 1e-08  | 1.1154125623136224 | 5.710318218277166e-09  | 1.1154125566033042 |          0.0           | 0.5551115123125782 |   1.4612111861626254   |\n",
            "| 1e-09  | 1.115412540109162  | 1.6494142274225965e-08 | 1.1154125566033042 |          0.0           | 55.51115123125782  |   53.49482853278262    |\n",
            "+--------+--------------------+------------------------+--------------------+------------------------+--------------------+------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complete your answer here if your exercise requires comments!"
      ],
      "metadata": {
        "id": "emUt4WfhjN7w"
      },
      "id": "emUt4WfhjN7w"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Richardson Extrapolation"
      ],
      "metadata": {
        "id": "g2fgYTph2Nop"
      },
      "id": "g2fgYTph2Nop"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Richardson extrapolation is a method used to improve the accuracy of numerical approximations, especially for finite difference methods. It involves computing a sequence of approximations using different step sizes, and then using these approximations to estimate the true value of the quantity being approximated.\n",
        "\n",
        "To form the matrix for computing differentiation up to some order of error using Richardson extrapolation, follow the following:\n",
        "\n",
        "We start by computing the initial approximations $D_{k,0}$ using the centred difference formula with step size $h$:\n",
        "\n",
        "$$D_{k,0} = \\frac{f(x+h) - f(x-h)}{2\\left(\\dfrac{h} {2^k}\\right)}$$\n",
        "\n",
        "Here's the complete algorithm for computing the matrix $D$:\n",
        "\n",
        "1. Choose a step size $h$.\n",
        "2. Compute the initial approximations $D_{k,0}$ using the formulas mentioned above.\n",
        "3. For $k$ from $1$ to $n$ and $j$ from $1$ to $k$, compute the entries $D_{k,j}$ using the recursive formula:\n",
        "$$D_{k,j} = \\frac{4^j D_{k-1,j-1} - D_{k-1,j}}{4^j - 1}$$\n",
        "\n",
        "4. The $n \\times n$ matrix $D$ with entries $D_{k,j}$ gives the true values of the derivatives up to order $n$.\n",
        "5. There is a variant of the algorithm that requires  results be within certain tolerance:\n",
        "If $\\left|R_{k+1, k+1}-R_{k, k}\\right|<\\varepsilon_{\\text {abs }}$, return the value $R_{k+1, k+1}$. If the loop finishes and nothing was returned, throw an exception indicating that Richardson extrapolation did not converge\n"
      ],
      "metadata": {
        "id": "pIPrc-ALnOYM"
      },
      "id": "pIPrc-ALnOYM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 2\n",
        "Complete the code below following the algorithm mentioned above for Richardson extrapolation."
      ],
      "metadata": {
        "id": "cGvfvFvV1-TG"
      },
      "id": "cGvfvFvV1-TG"
    },
    {
      "cell_type": "code",
      "source": [
        "def richardson(D, f, x, h, N_max, eps_abs):\n",
        "  \"\"\"\n",
        "  richardson: Function that improves the accuracy of numerical approximations\n",
        "\n",
        "  Parameters:\n",
        "      D: Function that computes the central divided difference.\n",
        "      f: Original mathematical function.\n",
        "      x: Point at which the derivative is approximated.\n",
        "      h: Step size for numerical differentiation.\n",
        "      N_max: Maximum number of iterations for Richardson extrapolation.\n",
        "      eps_abs: Absolute tolerance for convergence.\n",
        "\n",
        "  Returns:\n",
        "      Estimated derivative of at the point x.\n",
        "  \"\"\"\n",
        "  R = np.zeros((N_max + 1, N_max + 1))\n",
        "  R[0, 0] = D(f, x, h)\n",
        "  for i in range(1, N_max + 1):\n",
        "    p = h / (2 ** i)                                                     #change the value of h in each iteration\n",
        "    R[i, 0] = D(f, x, p)                                                 # calculate the first column in the drivative table\n",
        "\n",
        "    for j in range(1, i + 1):\n",
        "\n",
        "      R[i, j] = (4 ** j * R[i, j - 1] - R[i - 1, j - 1]) / (4 ** j - 1)  #Determine the D according to the pervious formula\n",
        "      if abs(R[i, j] - R[i - 1, j - 1]) < eps_abs:                       #Check convergence condition\n",
        "          return R[i, j]\n",
        "  raise Exception(\"Richardson extrapolation did not converge\")"
      ],
      "metadata": {
        "id": "lPRnkVIAnOwA"
      },
      "id": "lPRnkVIAnOwA",
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.set_printoptions(suppress=True)\n",
        "assert np.isclose(richardson(df_cdiv, np.sin, 1, 0.1, 5, 1e-12), 0.5403023058681484)"
      ],
      "metadata": {
        "id": "-Zm_Mgxwnq5J"
      },
      "id": "-Zm_Mgxwnq5J",
      "execution_count": 60,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}